{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network classifier for rare disease prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health claims data containing information on medical diagnoses, procedures and prescriptions can be used to identify undiagnosed rare disease patients. Ensemble decision tree approaches (such as `LightGBM` and `XGBoost`) are the most common machine learning approaches for tabular data. \n",
    "\n",
    "To optimise the performance of these models, feature engineering and feature selection is employed. Here, we implement a **Deep Neural Network classifier** optimised for the tabular datasets used in undiagnosed patient prediction in order to compare its performance to existing ensemble decision tree approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/cdsw/Data/EPI/imputed_dataset/test_epi_max_dd_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, fraction=1, tt_split = 0.3):\n",
    "  \n",
    "  ## Discard missing values\n",
    "  df = df.dropna()\n",
    "  df = df.sample(frac=fraction)\n",
    "\n",
    "  ## Define feature and target tables \n",
    "  targ_index = df.columns.get_loc('patient_type')\n",
    "  X = df.iloc[:, 1:targ_index].values\n",
    "  y = df.iloc[:, -1].values\n",
    "\n",
    "  ## Class balancing with random undersampling\n",
    "  from imblearn.under_sampling import RandomUnderSampler\n",
    "  rus = RandomUnderSampler(random_state=0, ratio=0.1)\n",
    "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "  \n",
    "  y_resampled= pd.get_dummies(y_resampled)\n",
    "\n",
    "  ## One hot encode gender \n",
    "  onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "  X_resampled = onehotencoder.fit_transform(X_resampled).toarray()\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=tt_split, \n",
    "                                                      shuffle= True)\n",
    "\n",
    "  ##Feature scaling\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test = scaler.fit_transform(X_test)\n",
    " \n",
    "  return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(df,1, 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.constraints import unit_norm\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output activation function\n",
    "\n",
    "`Softmax` function is used since outputs for this problem are mutually exclusive outputs.\n",
    "\n",
    "#### Hyperparameters and tuning\n",
    "The nn is built using the best hyperparameter values and architecture scanned with `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=16\n",
    "epochs=100\n",
    "n_neurons=256\n",
    "act_func='relu'\n",
    "act_func_out='softmax'\n",
    "opt = 'Adadelta'\n",
    "kernel_init = 'glorot_uniform'\n",
    "batch_size=1024\n",
    "drop_rate=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_neurons, input_shape=(X_train.shape[1],), activation=act_func,\n",
    "                   kernel_initializer = kernel_init))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(drop_rate))\n",
    "# Add as many hidden layers as specified in nl\n",
    "for layer in range(n_hidden):\n",
    "    model.add(Dense(n_neurons, activation=act_func, kernel_initializer = kernel_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop_rate))\n",
    "model.add(Dense(2, activation=act_func_out, kernel_initializer = kernel_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom metrics for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer=config.opt, metrics=['accuracy', f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = config.epochs, validation_data =(X_test, y_test),\n",
    "                    callbacks=[EarlyStopping(patience=50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "On training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred_train = model.predict(X_test) #same as predict_proba\n",
    "\n",
    "train_score = average_precision_score(y_test, nn_pred_train)\n",
    "print('train set average precision score:',train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/cdsw/Data/EPI/imputed_dataset/train_epi_max_dd_imputed.csv')\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "targ_index = df_test.columns.get_loc('patient_type')\n",
    "X_val = df_test.iloc[:, 1:targ_index]\n",
    "y_val = df_test.iloc[:, -1].values\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "X_val_1hot = onehotencoder.fit_transform(X_val).toarray()\n",
    "X_val = X_val_1hot\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "y_val= pd.get_dummies(y_val)\n",
    "nn_pred_val = model.predict(X_val)\n",
    "test_score = average_precision_score(y_val, nn_pred_val)\n",
    "print('test set average precision score:',test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
